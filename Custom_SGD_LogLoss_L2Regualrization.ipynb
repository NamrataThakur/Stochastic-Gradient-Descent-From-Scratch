{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# Don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#Don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=15, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=15, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log_loss',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log_loss', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 10 epochs took 0.06 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=15, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(eta0=0.0001, learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;,\n",
       "              random_state=15, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log_loss',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Weights : [[-0.42336692  0.18547565 -0.14859036  0.34144407 -0.2081867   0.56016579\n",
      "  -0.45242483 -0.09408813  0.2092732   0.18084126  0.19705191  0.00421916\n",
      "  -0.0796037   0.33852802  0.02266721]]\n",
      "SGD Weights Shape :  (1, 15)\n",
      "SGD Bias :  [-0.8531383]\n"
     ]
    }
   ],
   "source": [
    "print('SGD Weights :',clf.coef_)\n",
    "print('SGD Weights Shape : ', clf.coef_.shape)\n",
    "print('SGD Bias : ', clf.intercept_)\n",
    "\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros \n",
    "\n",
    "* Create a loss function \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector \n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept  <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept : <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights \n",
    "    - Compare the previous loss and the current loss, if it is not updating, then\n",
    "        stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dimension):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights as 1d array consisting of all zeros similar to the dimensions of row_vector\n",
    "    #initialize bias to zero\n",
    "    w = np.zeros_like(dimension)\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dimension=X_train[0] \n",
    "w,b = initialize_weights(dimension)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sigmoid_value = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    # We have been given two arrays y_true and y_pred and we have to calculate the logloss\n",
    "    #while dealing with numpy arrays we can use vectorized operations for quicker calculations as compared to using loops\n",
    "    #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "    #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/\n",
    "    loss_part1 = y_true*np.log10(y_pred)\n",
    "    loss_part2 = (1 - y_true)*np.log10(1-y_pred)\n",
    "    loss_sum = np.sum(loss_part1+loss_part2)\n",
    "    loss = (-1 * loss_sum)/len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#round off the value to 8 values\n",
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(np.round(loss,6)==0.076449)\n",
    "  return True\n",
    "true=np.array([1,1,0,1,0])\n",
    "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#make sure that the sigmoid function returns a scalar value, We can use dot function operation\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    z = (np.dot(np.transpose(w),x)+b)\n",
    "    sig_z = sigmoid(z)\n",
    "    #print(sig_z)\n",
    "    part1 = x*(y-sig_z)\n",
    "    part2 = (alpha*w)/N\n",
    "    dw = part1 - part2\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  assert(np.round(np.sum(grad_dw),5)==4.75684)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "grad_b=0.5\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "#sb should be a scalar value\n",
    "def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    z = np.dot(np.transpose(w),x) + b\n",
    "    sig_z = sigmoid(z)\n",
    "    db = y - sig_z\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(np.round(grad_db,4)==-0.3714)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0.5\n",
    "grad_b=0.1\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function used to compute predicted_y given the dataset X\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implementing the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train [for all data points in X_train] using pred function with updated weights\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test [for all data points in X_test] using pred function with updated weights\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # Compare previous loss and current loss, if loss is not updating then stop the process \n",
    "        # Return w,b , train_loss and test loss\n",
    "        \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    # Initialize the weights\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    \n",
    "    #write your code to perform SGD\n",
    "    N = len(X_train)\n",
    "    \n",
    "    #Iterating over each epoch\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #Iterating over a batch (batch_size=1 since we are implementating SGD)\n",
    "        for j in range(X_train.shape[0]):\n",
    "            \n",
    "            #Computing the gradient with respect to weights\n",
    "            dw = gradient_dw(X_train[j], y_train[j],w,b,alpha,N)\n",
    "            \n",
    "            #Computing the gradients with respect to bias\n",
    "            db = gradient_db(X_train[j],y_train[j],w,b)\n",
    "            \n",
    "            #Updating the weights using the computed gradient\n",
    "            w = w + eta0*dw\n",
    "            \n",
    "            #Updating the bias using the computed gradient\n",
    "            b = b + eta0*db\n",
    "            \n",
    "        #Predicting the train response variables using the new weights and bias    \n",
    "        y_train_pred = pred(w,b,X_train)\n",
    "        \n",
    "        #Getting the train loss on the train predicted response variables\n",
    "        train_loss_epoch = logloss(y_train,y_train_pred)\n",
    "        \n",
    "        #Checking if there is any improvement in the train loss. If not, terminate the training\n",
    "        if (len(train_loss) > 0) and (train_loss[-1] - train_loss_epoch == 0.0):\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            #Appending the new improved train loss in the list\n",
    "            train_loss.append(train_loss_epoch)\n",
    "            \n",
    "        #Predicting the test response variables using the new weights and bias\n",
    "        y_test_pred = pred(w,b,X_test)\n",
    "        \n",
    "        #Getting the test loss on the test predicted response variables\n",
    "        test_loss_epoch = logloss(y_test,y_test_pred)\n",
    "        \n",
    "        #Appending the new test loss in the list\n",
    "        test_loss.append(test_loss_epoch)  \n",
    "\n",
    "    return w,b,train_loss,test_loss,i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Logistic Regression model using Custom SGD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sUquz7LFEZ6E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "eta0=0.001\n",
    "N=len(X_train)\n",
    "epochs=20\n",
    "w,b,train_loss,test_loss,stopping_epoch=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41395277  0.19245295 -0.15005228  0.32635321 -0.22516684  0.58646736\n",
      " -0.42720457 -0.10028013  0.21483928  0.15555184  0.17881025 -0.01318754\n",
      " -0.06496902  0.36313889 -0.00985012]\n",
      "-0.9016735833888494\n"
     ]
    }
   ],
   "source": [
    "#print the value of weights w and bias b\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Weights :  [[-0.42336692  0.18547565 -0.14859036  0.34144407 -0.2081867   0.56016579\n",
      "  -0.45242483 -0.09408813  0.2092732   0.18084126  0.19705191  0.00421916\n",
      "  -0.0796037   0.33852802  0.02266721]]\n",
      "SGD Bias :  [-0.8531383]\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Weights : \",clf.coef_)\n",
    "print(\"SGD Bias : \",clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00941414,  0.0069773 , -0.00146193, -0.01509086, -0.01698014,\n",
       "          0.02630157,  0.02522026, -0.006192  ,  0.00556608, -0.02528942,\n",
       "         -0.01824166, -0.0174067 ,  0.01463468,  0.02461087, -0.03251733]]),\n",
       " array([-0.04853529]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "## <font color='red'>Goal of Project</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare <b>custom</b> implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in order of 10^-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The custom weights are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this grader function should return True\n",
    "#the difference between custom weights and clf.coef_ should be less than or equal to 0.05\n",
    "def differece_check_grader(w,b,coef,intercept):\n",
    "    val_array=np.abs(np.array(w-coef))\n",
    "    assert(np.all(val_array<=0.05))\n",
    "    print('The custom weights are correct')\n",
    "    return True\n",
    "differece_check_grader(w,b,clf.coef_,clf.intercept_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot your train and test loss vs epochs </font>\n",
    "\n",
    "plot epoch number on X-axis and loss on Y-axis and make sure that the curve is converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(stopping_epoch), train_loss, label = \"Train Loss\")\n",
    "plt.plot(range(stopping_epoch), test_loss, label = \"Test Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.title(\"LOSS PLOTS on Train and Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
